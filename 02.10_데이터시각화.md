# 02.10_데이터시각화

데이터 시각화: 데이터를 그래프나 차트와 같은 시각적 형식으로 표현하여 패턴, 추세, 인사이트를 효과적으로 전달하는 과정. 직관적으로 이해하기 쉽게 표현하는 기술이나 과정. 

통계를 수집해도 어떻게 해석을 하느냐가 ‘중요한 문제’

| **분류** | **설명** |
| --- | --- |
| **양적 데이터 시각화** | 수치형 데이터를 선 그래프, 막대 그래프, 히스토그램 등을 사용하여 표현합니다. |
| **질적 데이터 시각화** | 범주형 데이터를 파이 차트, 트리맵 등을 사용하여 그룹 간의 관계를 분석합니다. |
| **시계열 데이터 시각화** | 시간의 흐름에 따른 데이터 변화를 선 그래프, 영역 그래프 등으로 표현합니다. |
| **지리적 데이터 시각화** | 지도를 이용하여 위치 기반 데이터를 시각적으로 분석합니다. |

*극한(가까이 다가간다. 물리적으로 이동한다는 것은 시간을 필요로 함), 일상생활의 많은 활동(버스 정류장에 가다, 이동하다 등)이 시간에 기반하고 있음.* 

- matplotlib 라이브러리 활용

## 정형 데이터

= 표나 데이터베이스처럼 행(Row)과 열(Column)의 구조를 가진 체계적인 데이터

정형 데이터는 일반적으로 스키마(Schema)를 기반으로 구성되며, 각 열(Column)은 특정한 데이터 유형(예: 정수, 문자열, 날짜 등)을 가지며, 행은 개별적인 데이터 항목을 나타냅니다.

정형 데이터는 데이터베이스, 엑셀, CSV 파일, 관계형 데이터 저장소(RDBMS) 등에 저장되며, 쉽게 저장, 검색, 분석할 수 있도록 구조화되어 있다. 

- 정형 데이터 예시
    
    
    | **데이터 유형** | **예시** |
    | --- | --- |
    | **관계형 데이터베이스** | MySQL, PostgreSQL, SQLite 등의 데이터베이스 테이블 |
    | **엑셀 및 스프레드시트** | 직원 명단, 매출 데이터, 고객 목록 등의 정보가 있는 시트 파일 |

### 정형 데이터 불러오기

```jsx
import pandas as pd

# CSV 파일 불러오기
df_csv = pd.read_csv('data.csv')

# Excel 파일 불러오기
df_excel = pd.read_excel('data.xlsx')

# SQL 데이터베이스에서 불러오기 (**SQLite** 예시) 
import sqlite3
conn = sqlite3.connect('database.db') # SQL데이터를 pandas로 불러오기
df_sql = pd.**read_sql**("SELECT * FROM table_name", **conn**)
```

## 비정형 데이터

고정된 구조 없이 텍스트, 이미지, 영상, 오디오 등 다양한 형식으로 존재하는 데이터. 

구조가 아예 없는 게 아니라 (행렬과 같은 구조는 아니어도) 형식은 있음. 

비정형 데이터는 **고정된 스키마(schema)가 없어** **일정한 규칙으로 정리되지 않으며,** 데이터베이스의 관계형 테이블과 같은 체계적인 저장 방식이 아닌 **파일 시스템, 클라우드 스토리지, NoSQL 데이터베이스** 등에 저장되는 경우가 많습니다.

- 비정형 데이터와 정형 데이터의 가장 큰 구분: 행렬 구조가 있느냐

실제 데이터 세계의 대부분은 ‘비정형 데이터’

| 특징 | 설명 |  |
| --- | --- | --- |
| **고정된 형식이 없음** | 비정형 데이터는 일정한 구조나 형식을 가지지 않으며, 데이터를 구성하는 요소들이 명확하게 정리되지 않습니다. | 이메일 본문, 소셜 미디어 게시글, 블로그 글, 대화 기록 등 |
| **다양한 데이터 유형** | 비정형 데이터는 텍스트, 이미지, 오디오, 비디오 등 다양한 형식으로 존재하며, 각 데이터 유형마다 저장 방식과 처리 방법이 다릅니다. | - **텍스트 데이터** → 문서, 뉴스 기사, 논문, 챗봇 대화
- **이미지 데이터** → 사진, 인공지능(AI) 학습용 데이터
- **오디오 데이터** → 음성 녹음 파일, 팟캐스트, 통화 기록
- **비디오 데이터** → 유튜브 영상, 감시 카메라 영상 |
| **스키마가 없음** | 정형 데이터와 달리 비정형 데이터는 명확한 열(column)과 행(row)으로 정리되지 않으며, 저장 방식도 일관적이지 않습니다. | - **JSON, XML** 같은 **반정형 데이터 형식**으로 저장되기도 함
- 데이터베이스가 아니라 **파일 시스템에 개별 파일로 저장됨** |
| **대용량 데이터** | 비정형 데이터는 **방대한 양(빅데이터)**으로 생성되며, 실시간으로 증가하는 경우가 많습니다. | - **SNS(소셜 미디어)** → 트위터, 페이스북, 인스타그램에서 실시간으로 생성되는 데이터
- **CCTV** → 24시간 지속적으로 녹화되는 영상 데이터 |
| **분석이 복잡함** | 비정형 데이터는 일정한 규칙 없이 저장되므로 **분석이 어렵고,** 다양한 기술과 알고리즘이 필요합니다. |  |

<aside>
💡

**반정형 데이터?**
(Semi-Structured Data)는 정형 데이터처럼 일부 구조를 가지지만, 고정된 스키마 없이 유동적으로 저장되는 데이터이다. 
일반적으로 태그, 키-값 쌍 등의 형식으로 구성되며, 데이터베이스 테이블처럼 엄격한 구조는 없지만 특정한 규칙을 따른다. 
대표적인 예로 JSON, XML, YAML, 로그 파일 등이 있으며, 정형 데이터와 비정형 데이터의 중간 형태로 데이터 저장과 분석에 유연성을 제공한다. 

</aside>

- 사용 이유: 정형 데이터만으로는 포착할 수 없는 중요한 패턴과 의미를 해석 가능하게 하기 위해서

| **이유** | **설명** |
| --- | --- |
| **정형 데이터로 변환할 수 없는 정보 해석** | 텍스트, 이미지, 오디오, 영상 등의 비정형 데이터는 표 형태로 변환하기 어렵지만, 데이터 시각화를 통해 의미를 추출하고 해석할 수 있습니다. |
| **시각적 패턴과 관계 분석 가능** | 이미지 데이터에서는 특정 객체의 출현 빈도를 분석하거나, 영상 데이터에서는 행동 패턴을 탐지하는 등의 분석이 가능합니다.
이러한 비정형 데이터의 특성을 시각화하면 직관적으로 관계를 파악할 수 있습니다. |
- 사용 방법: 수집 → **전처리 → 변환** → 분석 → 저장
1. 비정형 데이터 수집 : 웹 스크래핑, API 요청, 파일 읽기 등의 방법
    
    ```jsx
    import requests  # HTTP 요청을 보내기 위한 라이브러리
    from bs4 import BeautifulSoup  # HTML 파싱을 위한 라이브러리
    
    # 웹페이지에서 텍스트 데이터 수집
    url = "https://example.com"  # 크롤링할 웹사이트 URL
    response = requests.get(url)  # 웹페이지 요청 (GET 방식)
    soup = BeautifulSoup(response.text, "html.parser")  # HTML 파싱
    
    # 페이지에서 모든 텍스트 추출
    text_data = soup.get_text()  # HTML에서 텍스트만 추출
    print(text_data[:500])  # 처음 500자 출력
    ```
    
2. API 이용한 데이터 수집 (JSON 데이터)
    
    ```jsx
    import requests  # HTTP 요청을 보내기 위한 라이브러리
    
    # 예제: 공공 데이터 API에서 JSON 데이터 가져오기
    api_url = "https://jsonplaceholder.typicode.com/posts"  # API 엔드포인트 URL
    response = requests.get(api_url)  # API 요청 (GET 방식)
    
    # JSON 데이터 변환
    data = response.json()  # 응답 데이터를 JSON 형식으로 변환
    
    # 처음 3개 게시글 출력
    for post in data[:3]:  # 처음 3개의 데이터만 반복
        print(f"제목: {post['title']}\n내용: {post['body']}\n")  # 제목과 내용을 출력
    ```
    
3. 파일에서 데이터 수집 (텍스트 파일)
    
    ```jsx
    with open("sample_text.txt", "r", encoding="utf-8") as file:  # 파일을 읽기 모드로 열기
        text_data = file.read()  # 파일 전체 내용을 문자열로 읽기
    
    print(text_data[:500])  # 처음 500자 출력
    ```
    
4. 파일에서 데이터 수집 (CSV 파일)
    
    ```jsx
    import pandas as pd
    
    # CSV 파일 읽기
    df = pd.read_csv("sample_data.csv")
    
    # 데이터 확인
    print(df.head())  # 처음 5개 행 출력
    ```
    
5. 파일에서 데이터 수집 (이미지 파일)
    
    ```jsx
    from PIL import Image
    import numpy as np
    
    # 이미지 불러오기
    image = Image.open("example.jpg")
    
    # NumPy 배열로 변환
    image_array = np.array(image) #정형화
    
    # 이미지 크기 출력
    print(image_array.shape)  # (높이, 너비, 채널 수)
    ```
    

### 비정형 데이터 전처리

1. 텍스트 데이터 정제
    
    ```jsx
    import re # 정규 표현식 사용을 위한 라이브러리
    
    # 불필요한 공백 및 특수문자 제거
    clean_text = re.sub(r"[^a-zA-Z0-9\s]", "", text_data)  # 영문, 숫자, 공백 제외한 문자 제거
    clean_text = re.sub(r"\s+", " ", clean_text).strip()  # 연속 공백 제거
    print(clean_text[:500])  # 정제된 텍스트 출력
    ```
    
2. 이미지 데이터 전처리
    
    ```jsx
    from PIL import Image  # 이미지 처리를 위한 라이브러리
    import numpy as np  # 행렬 연산을 위한 라이브러리
    
    # 이미지 불러오기
    image = Image.open("example.jpg")  # 이미지 파일 열기
    
    # 이미지 크기 조정
    resized_image = image.resize((256, 256))  # 256x256 크기로 변환
    
    # 이미지 데이터를 NumPy 배열로 변환
    image_array = np.array(resized_image)
    print(image_array.shape)  # (256, 256, 3) 형태 출력 (3은 RGB 채널)
    ```
    

### 비정형 데이터 변환 (정형 데이터로 변환)

1. 텍스트 데이터를 정형 데이터로 변환 (토큰화 및 감성 분석)
    
    ```jsx
    import pandas as pd  # 데이터 처리를 위한 pandas 라이브러리
    from textblob import TextBlob  # 감성 분석을 위한 TextBlob 라이브러리
    
    # 예제 텍스트 데이터
    clean_text = "I love programming. Python is amazing. Data science is fun."
    
    # 텍스트 데이터를 문장 단위로 분리
    sentences = clean_text.split(". ")
    
    # 감성 분석 후 정형 데이터로 변환
    df = pd.DataFrame({"문장": sentences})  # 문장을 데이터프레임으로 변환
    df["감성 점수"] = df["문장"].apply(lambda x: TextBlob(x).sentiment.polarity)  # 감성 분석 수행
    
    print(df.head())  # 첫 5개 행 출력
    ```
    
2. 이미지 데이터를 정형 데이터로 변환 (특정 벡터 추출)
    
    ```jsx
    import numpy as np  # 수치 연산을 위한 NumPy 라이브러리
    import pandas as pd  # 데이터 처리를 위한 pandas 라이브러리
    from skimage.feature import hog  # 이미지 특징 추출을 위한 HOG (Histogram of Oriented Gradients)
    from skimage import color, io  # 이미지 로딩 및 변환을 위한 라이브러리
    
    # 이미지 불러오기 (예제: 임의의 이미지 사용)
    image_path = "sample_image.jpg"
    image = io.imread(image_path)  # 이미지 로드
    gray_image = color.rgb2gray(image)  # 흑백(그레이스케일) 변환
    
    # HOG (Histogram of Oriented Gradients) 특징 추출
    hog_features = hog(gray_image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)
    
    # 데이터프레임으로 변환
    df_image = pd.DataFrame([hog_features])  # HOG 특징 벡터를 데이터프레임으로 변환
    print(df_image.head())  # HOG 특징 벡터 출력
    ```
    

### 비정형 데이터 분석

1. 텍스트 데이터 분석 (빈도 분석)
    
    ```jsx
    from collections import Counter  # 단어 빈도 분석을 위한 Counter 라이브러리
    
    # 단어 빈도수 계산
    words = clean_text.split()  # 공백을 기준으로 단어 분리
    word_counts = Counter(words)  # 단어 개수 계산
    
    # 정렬하여 상위 10개 단어 출력
    print(word_counts.most_common(10))  # 가장 많이 등장한 단어 10개 출력
    ```
    
2. 이미지 데이터 분석 (색상 분석)
    
    ```jsx
    import numpy as np  # 수치 연산을 위한 NumPy 라이브러리
    import matplotlib.pyplot as plt  # 데이터 시각화를 위한 Matplotlib 라이브러리
    
    # RGB 채널별 평균 색상 계산
    r_mean = np.mean(image[:, :, 0])  # Red 채널 평균
    g_mean = np.mean(image[:, :, 1])  # Green 채널 평균
    b_mean = np.mean(image[:, :, 2])  # Blue 채널 평균
    
    print(f"평균 색상 - R: {r_mean:.2f}, G: {g_mean:.2f}, B: {b_mean:.2f}")  # 평균 색상 출력
    
    # 시각화: RGB 색상 막대 그래프
    plt.bar(["Red", "Green", "Blue"], [r_mean, g_mean, b_mean], color=["red", "green", "blue"])
    plt.show()
    ```
    

### 비정형 데이터 불러오기 및 저장하기

(교재 보고 실습 해보기)

## Matplotlib

- Numpy, Pandas, Matplotlib은 파이썬 데이터 분석 3대장

Matplolib는 파이썬에서 다양한 형태의 차트와 그래프를 생성할 수 있도록 지원하는 기본적인 데이터 시각화 라이브러리.  

*MATLAB(Matrix labotatory)는 뉴 멕시코 대학에 의해 개발된 행렬용 수학 패키지*

Pandas, NumPy 등과 쉽게 연동되며, 데이터를 손쉽게 변환하여 시각화할 수 있습니다.
데이터 분석과 머신러닝 과정에서도 자연스럽게 통합되어 사용됩니다.

<aside>
💡

구글 코랩에서 **`plt.plot(x, y)`**만 사용해도 그래프가 출력되는데, **`plt.show()`**가 필요한가요?

Google Colab 환경에서는 **`plt.show()`** 없이도 자동으로 그래프가 렌더링됩니다. 

하지만 스크립트(.py) 실행 환경에서는 **`plt.show()`**를 써야 그래프가 표시됩니다.

또한, 여러 개의 그래프를 한 번에 그릴 때, **`plt.show()`**를 사용하면 원하는 위치에서 출력이 가능합니다.

</aside>

- 다양한 시각화 예제
    - 실습해보기…

### 막대 그래프 (Bar chart)

- 사용 이유: 범주형 데이터의 차이를 직관적으로 비교하여 패턴과 결론을 빠르게 도출하기 위해서

### 히스토그램 (Histogram)

연속형 데이터의 분포를 나타내기 위해 데이터를 구간(bin)으로 나누고, 각 구간의 빈도수를 막대로 표현하는 시각화 방법

| 특징 | 설명 |
| --- | --- |
| **빈도수(Frequency) 또는
확률 밀도(Probability Density)로
표현 가능** | 히스토그램은 기본적으로 각 구간 단위(bin)에 속한 데이터 개수를 빈도수로 나타냅니다.
그러나 **확률 밀도로 변환할 수도 있으며, 이 경우 히스토그램의 전체 면적이 1이 되도록 정규화됩니다.**
확률 밀도로 변환하면 **특정 구간 내 데이터가 분포할 확률**을 쉽게 분석할 수 있습니다. |
| **데이터의 분포 분석 가능** | 히스토그램을 사용하면 데이터의 **중심 경향(평균, 중앙값)과 퍼짐 정도(분산, 표준편차)를 빠르게 확인**할 수 있습니다.
또한, 데이터가 **한쪽으로 치우친 정도(왜도)나 뾰족한 정도(첨도)도 분석할 수 있어** 데이터의 **정규성(정규 분포 여부), 이상치(outlier) 유무** 등을 파악하는 데 유용합니다. |

사용 이유: 데이터의 분포(대칭성, 치우침, 봉우리 수 등)를 직관적으로 파악하기 위해서

1. 기본 히스토그램
    
    ```jsx
    import matplotlib.pyplot as plt
    import numpy as np
    
    # 예시를 위한 랜덤 데이터 생성 (정규 분포)
    data = np.random.randn(1000)  # 평균 0, 표준편차 1인 정규 분포에서 1000개의 데이터 추출
    
    # 기본 히스토그램 생성
    plt.hist(data, bins=20, color='skyblue', edgecolor='black') 
    plt.xlabel("Value")      # X축 라벨 설정 (데이터 범위)
    plt.ylabel("Frequency")  # Y축 라벨 설정 (빈도)
    plt.title("Basic Histogram")  # 그래프 제목 설정
    
    plt.show()
    ```
    
    random variable이란? 가능한 경우의 수 {앞면, 뒷면} → 시행 결과를 ‘수’로 바꿔놓는 것 {1, -1}
    
    확률변수에 대한 **확률 분포**
    
    확률분포 종류
    
    - 이산 확률 분포 → 확률 질량 함수 (주사위 던지기: P(X=1)=1/2 )
    - 연속 확률 분포 → 확률 밀도 함수

### 산점도 (Scatter plot)

연속 변수 간의 관계를 2차원 평면상에 점으로 표현하는 것

- 일반적으로 하나의 변수 값을 x축, 다른 변수 값을 y축에 배치하여 두 변수 간의 관계를 시각적으로 나타냅니다.
- 데이터가 특정 패턴을 형성하는 경우 변수 간의 상관관계를 파악할 수 있으며, 분포의 형태나 이상값(outlier)도 쉽게 확인할 수 있습니다.

{(1,2), (2,4), …, (5,5)} = {(x, f(x))} = “Graph”

```jsx
import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리 불러오기

# 데이터 정의
x = [1, 2, 3, 4, 5]  # X축 데이터 (변수 1)
y = [2, 4, 4, 6, 5]  # Y축 데이터 (변수 2)

# 산점도 생성
plt.scatter(x, y, color='blue', marker='o')  # X, Y 좌표에 해당하는 점을 파란색 원으로 표시
plt.xlabel("X-axis")  # X축 라벨 설정
plt.ylabel("Y-axis")  # Y축 라벨 설정
plt.title("Basic Scatter Plot")  # 그래프 제목 설정

# 그래프 표시
plt.show()  # 설정한 내용을 화면에 출력
```

### 박스플롯(Box Plot)

Box: 데이터의 중간 50%를 나타내는 사각형 모양의 요소

데이터의 분포, 중앙값(Median), 사분위수(Quartiles), 이상치(Outliers)를 시각적으로 나타낸다. 

박스(Box)와 수염(Whisker)으로 구성되며, 최소값, Q1, Q2, Q3, 최대값을 나타낸다. 

1. 기본 박스 플롯 그리기
    
    ```jsx
    import matplotlib.pyplot as plt  # 그래프를 그리기 위한 Matplotlib 라이브러리 불러오기
    import numpy as np  # 숫자 연산 및 난수 생성을 위한 NumPy 라이브러리 불러오기
    
    np.random.seed(42)  
    # random.seed()는 같은 output 도출. 초기값 다르게 넣기 (ex: 42)
    
    data = np.random.randn(100)  # 평균 0, 표준편차 1을 따르는 정규분포에서 난수 100개 생성
    
    plt.boxplot(data)  # 박스 플롯 생성
    plt.title("Basic Box Plot")  # 그래프 제목 설정
    plt.ylabel("Values")  # Y축 라벨 설정
    
    plt.show()  # 그래프 출력
    ```
    

1. 여러 개의 데이터 비교 (다중 박스 플롯)
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image.png)
    

### 고급 다중 그래프

하나의 Figure 내에서 여러 개의 그래프를 배치하여 다양한 데이터 관계를 동시에 시각화하는 기법입니다.

- 사용 이유: 하나의 Figure 내에서 여러 개의 그래프를 배치해 서로 다른 데이터 간의 관계를 한눈에 비교하고, 다각적인 분석을 하기 위해서

![고급다중그래프.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/%E1%84%80%E1%85%A9%E1%84%80%E1%85%B3%E1%86%B8%E1%84%83%E1%85%A1%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%91%E1%85%B3.png)

<aside>
💡

Figure가 뭐예요?
⇒ Figure는 Matplotlib에서 그래프가 그려지는 “캔버스” 또는 “창”을 의미합니다.

</aside>

1. 기본 다중 서브플롯

`plt.subplots()`를 사용하면 하나의 Figure 내에 여러 개의 그래프(서브플롯)를 배치할 수 있습니다.

```jsx
import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리 불러오기
import numpy as np  # 수학 연산과 난수 생성을 위한 라이브러리 불러오기

# 데이터 생성
x = np.linspace(0, 10, 100)  # 0에서 10까지 100개의 균등한 값을 생성
y1 = np.sin(x)  # 사인 함수 값 생성
y2 = np.cos(x)  # 코사인 함수 값 생성

# 서브플롯 설정 (2행 1열)
fig, axes = plt**.subplots**(**nrows=2, ncols=1,** figsize=(8, 6))  # 2개의 그래프를 세로로 배치, 크기는 8x6 인치

# 첫 번째 서브플롯 (사인 함수)
**axes[0]**.plot(x, y1, color='b', label='sin(x)')  # 첫 번째 서브플롯에 사인 그래프 추가 (파란색)
axes[0].set_title("Sine Function")  # 첫 번째 서브플롯 제목 설정
axes[0].set_xlabel("X-axis")  # 첫 번째 서브플롯의 X축 라벨 설정
axes[0].set_ylabel("Y-axis")  # 첫 번째 서브플롯의 Y축 라벨 설정
axes[0].legend()  # 범례 추가

# 두 번째 서브플롯 (코사인 함수)
**axes[1**].plot(x, y2, color='r', label='cos(x)')  # 두 번째 서브플롯에 코사인 그래프 추가 (빨간색)
axes[1].set_title("Cosine Function")  # 두 번째 서브플롯 제목 설정
axes[1].set_xlabel("X-axis")  # 두 번째 서브플롯의 X축 라벨 설정
axes[1].set_ylabel("Y-axis")  # 두 번째 서브플롯의 Y축 라벨 설정
axes[1].legend()  # 범례 추가

# 전체 레이아웃 조정
plt.tight_layout()  # 그래프 간 간격 자동 조정

# 그래프 출력
plt.show()  # 그래프 화면 출력
```

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%201.png)

1. 2x2
    
    ```jsx
    **fig, axes** = plt.subplots(**2, 2**, figsize=(10, 8))  # 2x2 서브플롯 생성, 크기는 10x8 인치
    
    # 선 그래프
    **axes[0, 0]**.plot(x, y1, color='b')  # 첫 번째 그래프 (선 그래프, 파란색)
    **axes[0, 0]**.set_title("Line Plot")  # 제목 설정
    
    # 산점도
    **axes[0, 1]**.scatter(x, y1, color='r')  # 두 번째 그래프 (산점도, 빨간색)
    **axes[0, 1]**.set_title("Scatter Plot")  # 제목 설정
    
    # 막대 그래프
    **axes[1, 0]**.bar(np.arange(5), [3, 1, 4, 1, 5], color='g')  # 세 번째 그래프 (막대 그래프, 초록색)
    **axes[1, 0]**.set_title("Bar Chart")  # 제목 설정
    
    # 히스토그램
    data = np.random.randn(1000)  # 정규 분포를 따르는 난수 1000개 생성
    **axes[1, 1]**.hist(data, bins=20, color='purple')  # 네 번째 그래프 (히스토그램, 보라색)
    **axes[1, 1]**.set_title("Histogram")  # 제목 설정
    
    # 그래프 출력
    plt.tight_layout()  # 그래프 간 간격 자동 조정
    plt.show()  # 그래프 화면 출력
    ```
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%202.png)
    
2. 공유 축 설정 (공통 X축 또는 Y축 사용, `sharex` 활용)
3. 불규칙한 형태의 서브플롯 (`gridspec` 활용)

### 벤 다이어그램(Venn diagram)

[]: 리스트 (수학 - 행렬)

(): 튜플 (수학 - O)

{”a”:”b”} : 딕셔너리 (수학 - X)

{}: 집합(set) (수학 - O); 중복 허용X

# Seaborn

*Numpy → pandas*

*matplotlib → seaborn*

통계적 데이터 시각화를 쉽게 구현할 수 있도록 고급 스타일과 기능을 제공하는 파이썬 데이터 시각화 라이브러리

Pandas의 데이터프레임과 잘 호환되도록 설계되어 있어 복잡한 시각화도 간단한 코드로 표현할 수 있습니다.

- Seaborn은 Matplotlib를 기반으로, 위에서 동작하며 **더 직관적이고 보기 좋은 스타일의 그래프**를 생성하도록 다양한 기능을 제공합니다. Matplotlib을 직접 사용할 때보다 더 간결한 코드로 높은 수준의 시각화를 만들 수 있습니다.
- 데이터를 단순히 시각화하는 것을 넘어, 통계적 특성을 강조하는 기능. 회귀선(regression line), 밀도 추정(density estimation), 범주형 데이터 표현 등을 지원함.

<aside>
💡

회귀선(regression line), 밀도 추정(density estimation)이 뭐예요?

![[https://www.toppr.com/guides/fundamentals-of-business-mathematics-and-statistics/correlation-and-regression/regression-lines/](https://www.toppr.com/guides/fundamentals-of-business-mathematics-and-statistics/correlation-and-regression/regression-lines/)](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%203.png)

[https://www.toppr.com/guides/fundamentals-of-business-mathematics-and-statistics/correlation-and-regression/regression-lines/](https://www.toppr.com/guides/fundamentals-of-business-mathematics-and-statistics/correlation-and-regression/regression-lines/)

회귀선 (Regression Line)은 **두 변수 간의 관계**를 수학적 모델**(선형 회귀 등)로 표현한 직선**으로, 데이터의 경향성을 나타냅니다.

![[https://en.wikipedia.org/wiki/Density_estimation](https://en.wikipedia.org/wiki/Density_estimation)](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%204.png)

[https://en.wikipedia.org/wiki/Density_estimation](https://en.wikipedia.org/wiki/Density_estimation)

밀도 추정 (Density Estimation)은 데이터의 분포를 **부드러운 곡선으로 표현하는 기법**으로, 히스토그램보다 연속적인 확률 분포를 보여줍니다.

</aside>

```jsx
import seaborn as sns  # Seaborn 라이브러리 임포트
import matplotlib.pyplot as plt  # 그래프 출력을 위한 Matplotlib 임포트. Seaborn 그래프를 출력할 때 Matplotlib을 활용하기 위해 불러옴
```

```jsx
import pandas as pd  # 데이터프레임을 사용하기 위한 Pandas 임포트

# 샘플 데이터 생성
df = pd.DataFrame({
    "x": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    "y": [2, 4, 1, 5, 7, 8, 5, 9, 10, 6]
})

# Seaborn을 이용한 산점도 생성
**sns.scatterplot(x="x", y="y", data=df)**

# 그래프 제목 설정
**plt.title**("Basic Scatter Plot with Seaborn")

# 그래프 출력
plt.show()
```

### 범주형 데이터(Categorical Data)

범주형 데이터는 **정해진 그룹이나 레이블을 가지는 데이터**를 의미하며, 일반적으로 명목형(Nominal) 또는 순서형(Ordinal) 변수로 구성됩니다.

사용 이유: 그룹 간 차이와 패턴을 직관적으로 비교·분석하기 위해서

1. 막대 그래프
    
    ```jsx
    # 기본 범주형 데이터 시각화 (막대 그래프)
    sns.barplot(x="Category", y="Value", data=data)  # Category 별 평균 Value를 나타내는 막대 그래프 생성
    plt.title("Basic Categorical Bar Plot")  # 그래프 제목 설정
    plt.show()  # 그래프 출력
    ```
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%205.png)
    
2. 박스 플롯
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%206.png)
    
3. 바이올린 플롯 (데이터 밀도와 분포 확인)
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%207.png)
    
4. 스트립 플롯 (데이터 포인트 개별 시각화)
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%208.png)
    

### 연속형 데이터(Continous data)

| **특징** | **설명** |
| --- | --- |
| 수치적이며 연속된 값 | 연속형 데이터는 정해진 범위 내에서 실수 값을 가질 수 있으며, 소수점 단위까지 표현할 수 있습니다.
- 예: 키(175.3cm), 온도(23.6℃), 시간(12.45초), 연봉(5,530만 원) |
| **측정 단위에 따라 세분화 가능** | 특정한 측정 단위(소수점, 밀리미터, 밀리초 등)로 더 정밀하게 표현할 수 있습니다.
- 예: 거리(1.23km vs. 1.2345km), 무게(65.4kg vs. 65.432kg) |
| **연속적인 변화와 경향 분석 가능** | 시간에 따른 변화(시계열 데이터), 특정 변수의 증가/감소 경향성을 분석할 수 있습니다.
- 예: 주식 가격 변화, 기온 변화, 연령대별 평균 소득 증가율 |
| **Seaborn에서 다양한 형태의 시각화 가능** | 히스토그램(histplot), 커널 밀도 추정(kdeplot), 선 그래프(lineplot), 산점도(scatterplot) 등 다양한 방법으로 표현할 수 있습니다. |

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%209.png)

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2010.png)

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2011.png)

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2012.png)

![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2013.png)

### 관계형 데이터(Relational data)

수치형 데이터 간의 상관관계를 분석하는 데 활용됨. 

인과관계 : 원인 - 결과

상관관계 : 관련이 있는가?

1. Seaborn 관계 데이터 기본 시각화
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2014.png)
    
2. 회귀선이 포함된 산점도
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2015.png)
    
3. 다중 변수 간 관계 분석 (Pair plot)
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2016.png)
    

### 시계열 데이터 (Time Series Data)

시계열 데이터(Time Series Data)는 시간의 흐름에 따라 일정한 간격(초, 분, 시간, 일, 월, 연 등) 으로 측정된 연속적인 데이터입니다.

대표적인 시계열 데이터 예시

| **예시** | **설명** | **시간 축 (독립 변수)** | **관측 값 (종속 변수)** |
| --- | --- | --- | --- |
| **주식 가격 변화** | 특정 기업의 주가가 1일 단위(혹은 분 단위, 초 단위 등)로 측정된 시계열 데이터 | 1월 1일, 1월 2일, 1월 3일 등 (또는 특정 시간 간격) | 종가(Closing Price), 시가, 고가, 저가 등 |
| **날씨 (기온, 강수량, 습도)** | 특정 지역의 기온, 강수량, 습도를 일정 주기(일, 시간 등)로 기록한 기상 관측 데이터 | 1일 차, 2일 차, 3일 차 … (일 단위)
0시, 1시 … (시간 단위) | 기온(℃), 강수량(mm), 습도(%) 등 |
| **웹사이트/앱 접속 트래픽** | 웹사이트나 앱에 접속한 사용자 수가 일정 간격(분, 시간 등)으로 기록된 데이터 | 00시 ~ 01시, 01시 ~ 02시 … (시간 단위) | **접속자 수, 페이지뷰(PV), 클릭 수 등** |
| **IoT 센서 데이터** | 제조 공장, 스마트 홈 등에서 센서를 통해 수집되는 온도, 습도, 진동, 소음 등 각종 환경 데이터 | 초 단위(0초, 1초, 2초 …), 혹은 특정 주기(분, 시간 등) | 센서 측정값 (온도, 진동 세기, 소음 레벨, 습도 등) |
| **환자 생체 데이터(의료)** | 병원, 중환자실 등에서 환자의 심박수, 혈압, 체온 등을 일정 간격으로 측정한 모니터링 데이터 | 0분, 1분, 2분 … (분 단위) |  |

사용 이유: 시계열 데이터를 사용하는 이유는 시간에 따른 변화를 분석하여 미래를 예측하고, 불확실성을 줄이기 위해서입니다.

사용 방법: 날짜별 값을 생성한 후, 선 그래프로 시각화하고, 이동 평균을 적용해 추세를 분석

1. 시계열 데이터 생성 및 기본 시각화
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2017.png)
    
2. 이동 평균(Moving Average) 시각화
현재치 뿐만 아니라, 최근치들의 평균. 
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2018.png)
    
3. 시계열 데이터의 이상치 탐지 (IQR 활용)
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2019.png)
    

### 리샘플링(Resampling)

시계열 데이터의 분석을 위해 데이터의 시간 간격을 재조정하여 다운샘플링(축소)하거나 업샘플링(확대)하는 과정. 

sample은 **전체 데이터 집합의 특성을 대표할 수 있는 부분 집합**이고, -ing는 규칙 동사의 현재 분사를 만드는 데 쓰이는 접미사입니다.

![**Daily Data**: 원본 데이터로, 30일 동안의 변화를 나타냅니다.
**Weekly Data**: 7일 단위로 평균을 내어 다운샘플링한 데이터입니다. 데이터 포인트가 줄어들면서 세부적인 변동이 사라졌습니다.
**Hourly Data**: 1일을 24시간으로 세분화하여 업샘플링한 데이터입니다. 선형 보간을 사용하여 부드러운 변화를 만들어냈습니다.](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2020.png)

**Daily Data**: 원본 데이터로, 30일 동안의 변화를 나타냅니다.
**Weekly Data**: 7일 단위로 평균을 내어 다운샘플링한 데이터입니다. 데이터 포인트가 줄어들면서 세부적인 변동이 사라졌습니다.
**Hourly Data**: 1일을 24시간으로 세분화하여 업샘플링한 데이터입니다. 선형 보간을 사용하여 부드러운 변화를 만들어냈습니다.

**Synthetic data** are artificially generated data rather than produced by real-world events

통계학에서 : 표본 공간

| **개념** | **설명** |
| --- | --- |
| **다운샘플링(Downsampling)** | 기존 데이터보다 더 긴 시간 간격으로 데이터를 집계하는 과정입니다.
예를 들어, 초 단위 데이터를 분, 시간, 일 단위로 변환하는 것이 다운샘플링에 해당합니다.
평균, 합계, 최대값, 최소값, 중앙값 등의 **집계 함수**를 사용하여 데이터를 요약합니다. |
| **업샘플링(Upsampling)** | 기존 데이터보다 더 짧은 시간 간격으로 데이터를 확장하는 과정입니다.
예를 들어, 일 단위 데이터를 시간, 분, 초 단위로 변환하는 것이 업샘플링에 해당합니다.
**보간(Interpolation) 기법**을 사용하여 누락된 값을 보충할 수 있습니다. |
| **고정된 시간 간격으로 변환** | 리샘플링은 데이터의 시간 간격을 일정한 주기로 변경하는 것이므로, 데이터가 고정된 간격으로 정렬됩니다. |
| **집계(Aggregation)와
보간(Interpolation)** | - 다운샘플링에서는 **시간 간격이 길어지므로, 평균·합계·최대값 등 집계(Aggregation) 연산이 필요합니다.**
- 업샘플링에서는 **시간 간격이 짧아지므로, 보간(Interpolation) 기법을 활용해 새로운 값을 생성합니다.** |

reshape, resize, resample → ‘다시’라는 의미보다는 변환(transform)의 의미. 

Resampling을 사용하는 이유는 불규칙한 시계열 데이터를 일정한 간격으로 정리하여 패턴과 트렌드를 명확하게 파악하기 위해서입니다.

<aside>
💡

리샘플링은 **단순히 시간 간격을 변경하는 것이 아니라, 데이터의 패턴과 트렌드를 강조하고 해석의 명확성을 높이는 과정**입니다.

데이터를 분석 목적에 맞게 변환하여 의미 있는 인사이트를 도출하고, 더 효과적인 데이터 시각화를 구현할 수 있습니다.

</aside>

- 사용방법: Pandas의 `.resample()` 메서드를 사용하여 시계열 데이터를 특정 시간 간격으로 변환하고, 다운샘플링(축소) 시 집계 함수, 업샘플링(확대) 시 보간법을 적용하여 활용

```jsx
             datetime  value
0 2024-01-01 00:00:00     88
1 2024-01-01 03:00:00     75
2 2024-01-01 06:00:00     87
3 2024-01-01 09:00:00     38
4 2024-01-01 12:00:00     98
```

1. 다운 샘플링

```jsx
# 하루 단위로 다운샘플링 (평균 값 사용)
df_daily = df.**resample("D")**.mean()  # 'D'는 하루 단위 리샘플링을 의미

# 데이터 확인
print(df_daily.head())  # 다운샘플링된 데이터 확인
```

1. 업 샘플링

```jsx
# 1시간 단위로 업샘플링 (보간 없이 NaN 유지)
df_hourly = df.**resample("h")**.asfreq()  # 'h'는 1시간 단위 리샘플링을 의미

# 데이터 확인
print(df_hourly.head().reset_index())  # 업샘플링된 데이터 확인
```

1. 업샘플링 후 보간 (Interpolation)

| **보간법** | **사용법** | **설명** | **특징** |
| --- | --- | --- | --- |
| **선형 보간
(Linear Interpolation)** | df.interpolate(method="linear") | 두 인접한 값 사이를 직선으로 연결하여 NaN 값을 채움 | 데이터가 **일정한 증가/감소 경향**을 가질 때 유용 |
| **전방 채우기
(Forward Fill, ffill)** | df.interpolate(method="ffill") 또는 df.fillna(method="ffill") | NaN 값을 바로 이전 값으로 채움 | **이전 값이 유지되는 데이터** (예: 센서 데이터, 주식 가격)에서 사용 |
| **후방 채우기
(Backward Fill, bfill)** | df.interpolate(method="bfill") 또는 df.fillna(method="bfill") | NaN 값을 바로 다음 값으로 채움 | **빠른 응답이 필요한 데이터**에서 사용 |
| **다항식 보간
(Polynomial Interpolation)** | df.interpolate(method="polynomial", order=n) | 다항식 회귀를 이용하여 보간 | 곡선형 패턴이 있는 경우 적합하지만, **과적합 위험** 존재 |
| **스플라인 보간
(Spline Interpolation)** | df.interpolate(method="spline", order=n) | 스플라인 보간법을 이용하여 부드러운 곡선으로 채움 | **곡선형 패턴이 자연스러워야 할 때** 유용 |
| **시간 보간
(Time Interpolation)** | df.interpolate(method="time") | 시간 간격을 고려하여 보간 | **시계열 데이터에서 시간 흐름을 유지**할 때 유용 |
| **제곱 보간
(Quadratic Interpolation)** | df.interpolate(method="quadratic") | 2차 곡선(포물선) 형태로 보간 | 데이터가 **곡선 형태로 변화**하는 경우 적합 |
- 리샘플링을 활용한 데이터 시각화
    
    ![image.png](02%2010_%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%A8%E1%84%92%E1%85%AA%2019649f33ba5d807c9c9ed9a62d3953df/image%2021.png)
    

### 이동평균

데이터 시각화에서 이동평균은 시계열 데이터의 변동성을 완화하고 장기적인 추세를 파악하기 위해 일정 구간의 평균값을 계산하는 기법을 의미한다. 

1. 단순 이동평균 (SMA: Simple Moving Average)
    
    ```jsx
    # 데이터: 10, 20, 30, 40, 50
    첫 번째 이동평균: (10 + 20 + 30) / 3 = 20.0
    두 번째 이동평균: (20 + 30 + 40) / 3 = 30.0
    세 번째 이동평균: (30 + 40 + 50) / 3 = 40.0
    ```
    
2. 지수 이동평균 (EMA: Exponential Moving Average)
    
    지수 이동평균은 **최근 데이터에 더 높은 가중치**를 부여하여, 변동에 더 민감하게 반응하는 방식입니다.
    
    ```jsx
    # 데이터: 10, 20, 30, 40, 50
    첫 번째 EMA: 10.0 (초기값)
    두 번째 EMA: (20 * α) + (10 * (1 - α)) = 15.0
    세 번째 EMA: (30 * α) + (15 * (1 - α)) = 22.5
    네 번째 EMA: (40 * α) + (22.5 * (1 - α)) = 31.3
    다섯 번째 EMA: (50 * α) + (31.3 * (1 - α)) = 40.6
    ```
    
3. 가중 이동평균 (WMA: Weighted Moving Average)
    
    가중 이동평균은 **최근 값에 더 큰 가중치를 부여**하되, 지수 이동평균보다 **선형적인 방식**으로 가중치를 적용하는 방법입니다.
    
    ```jsx
    # 데이터: 10, 20, 30, 40, 50
    첫 번째 WMA: (10 * 0.1) + (20 * 0.3) + (30 * 0.6) = 25.0
    두 번째 WMA: (20 * 0.1) + (30 * 0.3) + (40 * 0.6) = 36.0
    세 번째 WMA: (30 * 0.1) + (40 * 0.3) + (50 * 0.6) = 46.0
    ```
    

- 사용 방법

이동평균은 Pandas의 `.rolling()` 메서드와 `.ewm()` 메서드를 사용하여 계산할 수 있습니다.

1. 단순 이동평균 
    
    ```jsx
    df["SMA"] = df["value"].rolling(window=윈도우_크기).mean()
    ```
    
2. 지수 이동평균
    
    ```jsx
    df["EMA"] = df["value"].ewm(span=윈도우_크기, adjust=False).mean()
    ```
    
3. 가중 이동평균
    
    ```jsx
    weights = np.arange(1, 윈도우_크기 + 1)  # 가중치 배열 생성
    df["WMA"] = df["value"].rolling(window=윈도우_크기).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)
    ```
    

⇒ 금융 데이터에서 많이 쓰임. (+헬스데이터 등)

### 금융 데이터 (Financial Data)

- 미국 시장 API 잘 돼있어서, 단타(?) 치기 좋음.
- 우리나라에선 하기 어려움

로보 어드바이저: AI가 금융 종목 추천해줌. 

금융과 AI의 만남은 ‘최고’ (우리나라는 핀테크 규제가 좀 있지만, 세계적으로는)

나스닥에 주식 데이터 API로 쓸 수 있음. (단, 무료는 아님)

토스: 호가 안보여줌. 로빈후드식 방식. 

로빈후드 : 수수료 없음. 

금융 데이터는 주식 가격, 거래량, 환율, 금리 등 금융 시장에서 발생하는 시계열 기반의 데이터를 의미합니다. 

금융 데이터는 **시간의 흐름에 따라 변동하는 연속형 데이터**로, 시장 움직임을 분석 및 예측하는 데 중요한 역할을 합니다.

금융 데이터는 크게 **가격 데이터**와 **거래 데이터**로 구분할 수 있습니다.

금융 데이터를 사용하는 이유는 자본의 흐름을 분석하여 경제 활동을 예측하고 최적의 의사결정을 내리기 위해서이다. 

.

.

. (예제 따라 해보기)

# Scipy

Numpy를 기반으로 구축되었으며, 다양한 수학적 함수와 알고리즘을 제공하여 과학, 공학, 데이터 분석 분야에서 폭넓게 활용된다. 

| **특징** | **설명** |
| --- | --- |
| **NumPy 기반의 확장성** | SciPy는 NumPy 배열을 기반으로 동작하며, NumPy의 연산을 더욱 확장하여 다양한 과학 계산을 지원합니다. |
| **고급 수학 및 최적화 기능 제공** | 함수 최적화, 최소제곱법, 회귀 분석 등 수학적 최적화 기능을 제공합니다. |
| **신호 처리 기능 포함** | 푸리에 변환(FFT), 필터링, 웨이브렛 변환 등 신호 처리 기능을 제공합니다. |
| **통계 및 확률 분포 분석 지원** | 다양한 확률 분포, 샘플링, 가설 검정, 회귀 분석 기능이 포함되어 있습니다. |
| **선형 대수 연산 강화** | 희소 행렬 연산, 고유값 분해, LU 분해 등 선형 대수 기능을 제공합니다. |
| **보간 및 회귀 분석 지원** | 데이터 사이 값을 예측하는 보간(interpolation) 및 다항 회귀 분석 기능이 포함됩니다. |
| **미적분 및 적분 연산 가능** | 수치 적분(quad, dblquad 등) 및 미분 방정식(ODE) 해결 기능을 제공합니다. |
- **푸리에 변환 (Fourier Transform)**: 신호나 데이터를 개별 주파수 성분으로 변환하여 주파수 도메인에서 분석하는 수학적 기법입니다.
- **웨이브렛 변환 (Wavelet Transform)**: 신호를 시간과 주파수 정보가 결합된 다중 해상도로 변환하여 분석하는 기법입니다.
- **고유값 분해 (Eigenvalue Decomposition, EVD)**: 행렬을 고유값과 고유벡터의 곱으로 분해하여 데이터의 주요 방향과 패턴을 분석하는 방법입니다.
- **LU 분해 (LU Decomposition)**: 행렬을 하삼각행렬(L)과 상삼각행렬(U)의 곱으로 분해하여 연립방정식 풀이 및 행렬 연산을 수행하는 방법입니다.

파이토치나 텐서플로우로 그 위에서 짜는 게 아니라, C++로 어떻게 짜냐

판을 바꾸고 싶으면, 앞으로 공부하는 것을 C++로 구현하는 것도 고민해보면 좋음. 

자바는 애매함. 퍼포먼스 측면에서 (메모리 관리 직접) 따라가지 못함. (비용 = 오버헤드)

- Scipy 주요 모듈 구성

| **모듈** | **설명** |
| --- | --- |
| **`scipy.optimize`** | 함수 최적화 및 최소화, 비선형 방정식 해 찾기 기능을 포함합니다. |
| **`scipy.stats`** | 확률 분포, 통계 분석, 가설 검정 및 확률 밀도 함수(PDF, CDF) 등을 제공합니다. |
| **`scipy.linalg`** | 선형 대수 연산을 수행하며, NumPy의 `numpy.linalg`보다 더 확장된 기능을 제공합니다. |
| **`scipy.fft`** | 빠른 푸리에 변환(FFT) 기능을 제공하여 신호 및 주파수 분석에 사용됩니다. |
| **`scipy.integrate`** | 수치 적분 및 미분 방정식(ODE) 해결 기능을 포함합니다. |
| **`scipy.interpolate`** | 데이터 보간(interpolation) 기능을 제공하여 연속적인 값 추정에 사용됩니다. |
| **`scipy.spatial`** | 공간 데이터 처리 기능을 포함하며, 최근접 이웃 검색(KDTree), 거리 계산 등을 제공합니다. |
| **`scipy.signal`** | 신호 처리 관련 기능(필터링, 컨볼루션, 주파수 분석 등)을 제공합니다. |
| **`scipy.sparse`** | 희소 행렬(Sparse Matrix) 연산 기능을 포함하여 대규모 행렬 연산을 최적화합니다. |
| **`scipy.ndimage`** | 이미지 처리 기능을 제공하며, 다양한 필터링 및 변환 기능을 포함합니다. |

- Scipy와 Numpy 차이점

| **모듈** | **설명** |
| --- | --- |
| **`scipy.optimize`** | 함수 최적화 및 최소화, 비선형 방정식 해 찾기 기능을 포함합니다. |
| **`scipy.stats`** | 확률 분포, 통계 분석, 가설 검정 및 확률 밀도 함수(PDF, CDF) 등을 제공합니다. |
| **`scipy.linalg`** | 선형 대수 연산을 수행하며, NumPy의 `numpy.linalg`보다 더 확장된 기능을 제공합니다. |
| **`scipy.fft`** | 빠른 푸리에 변환(FFT) 기능을 제공하여 신호 및 주파수 분석에 사용됩니다. |
| **`scipy.integrate`** | 수치 적분 및 미분 방정식(ODE) 해결 기능을 포함합니다. |
| **`scipy.interpolate`** | 데이터 보간(interpolation) 기능을 제공하여 연속적인 값 추정에 사용됩니다. |
| **`scipy.spatial`** | 공간 데이터 처리 기능을 포함하며, 최근접 이웃 검색(KDTree), 거리 계산 등을 제공합니다. |
| **`scipy.signal`** | 신호 처리 관련 기능(필터링, 컨볼루션, 주파수 분석 등)을 제공합니다. |
| **`scipy.sparse`** | 희소 행렬(Sparse Matrix) 연산 기능을 포함하여 대규모 행렬 연산을 최적화합니다. |
| **`scipy.ndimage`** | 이미지 처리 기능을 제공하며, 다양한 필터링 및 변환 기능을 포함합니다. |

- 사용 방법: 필요한 모듈을 불러온 후, 최적화(`optimize.minimize`), 통계 분석(`stats.norm`), 신호 처리(`fft`), 적분(`integrate.quad`), 선형 대수 연산(`linalg.eig`) 등의 기능을 함수 호출을 통해 사용

1. 최적화(Optimization) - 함수 최솟값 찾기
    
    ```jsx
    import numpy as np  # NumPy 라이브러리 불러오기
    import scipy  # SciPy 라이브러리 불러오기
    from scipy import optimize, stats, signal, interpolate, integrate, linalg  # SciPy의 주요 모듈 임포트
    
    # 예제 함수 정의 (y = x^2 + 5)
    def func(x):  # 최적화할 함수 정의
        return x**2 + 5  # 입력값 x에 대해 x^2 + 5 반환
    
    # 최솟값 찾기 (x = 0에서 최소)
    result = optimize.minimize(func, x0=3)  # 초기값 x0=3에서 함수의 최소값 찾기
    
    # 결과 확인
    print("최적화 결과:", result.x)  # 최적화된 x 값 출력
    ```
    
    최적화 결과: [-2.83269601e-08]
    
    → 최적화 결과인 `2.83269601e-08`은 `0`에 매우 가까운 값으로, 수치 연산 오차로 인해 정확히 `0`이 아닌 작은 값이 반환됨. 즉, 최적화 알고리즘이 정확히 `x=0`을 찾았음을 의미함
    
2. **통계 분석 (Statistics) - 정규 분포 생성 및 분석**
    
    SciPy의 `scipy.stats` 모듈을 사용하면 다양한 확률 분포를 생성하고 분석할 수 있습니다.
    
    ```jsx
    # 평균이 0, 표준편차가 1인 정규 분포에서 난수 5개 생성
    random_samples = stats.norm.rvs(loc=0, scale=1, size=5)  # 정규 분포에서 랜덤 값 5개를 생성
    
    # 정규 분포의 확률 밀도 함수(PDF) 계산
    x = np.linspace(-3, 3, 100)  # -3부터 3까지 100개의 균등한 값을 생성
    pdf_values = stats.norm.pdf(x, loc=0, scale=1)  # 평균 0, 표준편차 1의 정규 분포에서 확률 밀도 계산
    
    # 결과 확인
    print("정규 분포 샘플:", random_samples)  # 생성된 정규 분포 샘플을 출력
    
    ```
    
3. **신호 처리 (Signal Processing) - 푸리에 변환 (FFT)**
    
    ```jsx
    from scipy.fft import fft  # scipy의 FFT 함수를 불러옴
    
    # 신호 데이터 생성 (5Hz의 사인파)
    t = np.linspace(0, 1, 500)  # 0초부터 1초까지 500개의 균일한 샘플을 생성
    signal = np.sin(2 * np.pi * 5 * t)  # 주파수 5Hz의 사인파를 생성
    
    # 푸리에 변환 적용
    fft_result = **fft(**signal)  # 신호 데이터에 대해 푸리에 변환 수행
    
    # 결과 확인
    print("푸리에 변환 결과(일부):", fft_result[:5])  # 푸리에 변환 결과의 일부(처음 5개 값)를 출력
    ```
    
    → DA(Digital Analog 변환), 데이터 전처리에서 ‘푸리에 변환’이 쓰이기도 함. 
    
4. **수치 적분 (Integration) - 정적분 계산**
    
    ```jsx
    # 적분할 함수 정의 (y = x^2)
    def func(x):  # 함수 정의 시작
        return x**2  # 입력값 x에 대해 x^2을 반환
    
    # 0부터 3까지 정적분 계산
    integral, error = integrate.quad(func, 0, 3)  # func 함수를 [0, 3] 구간에서 정적분하고 오차도 반환
    
    # 결과 출력
    print("정적분 결과:", integral)  # 계산된 정적분 결과를 출력
    ```
    

1. 선형 대수(Linear Algebra) - 행렬 연산 및 고유값 분해 
    
    ```jsx
    A = np.array([[4, -2], [1, 1]])  # 2x2 행렬 A를 생성
    
    eigenvalues, eigenvectors = linalg.eig(A)  # 행렬 A의 고유값과 고유벡터를 계산
    
    print("고유값:", eigenvalues)  # 계산된 고유값을 출력
    print("고유벡터:\n", eigenvectors)  # 계산된 고유벡터를 출력
    ```
    

### 정규 분포 (Normal Distribution)

- AI 세상은 ‘확률 분포’로 이뤄짐. ‘실제 세계와 가상 세계의 간극을 어떻게 메울 것이냐’가 AI 개발자가 실현해야 할 일. 현실 세계를 가상 세계에 거의 완벽하게 구현하면, 예측이 맞아떨어진다(?)

.

.

(복습)

### 기술 통계 (Descriptive Statistics)

1. **중앙 경향성 (Central Tendency)**
    
    
    | 구분 | 설명 |
    | --- | --- |
    | **평균(Mean)** | 데이터 값의 합을 데이터 개수로 나눈 값으로, 대표적인 중심 경향성 지표입니다. |
    | **중앙값(Median)** | 데이터를 정렬했을 때 가운데 오는 값으로, 이상값(Outlier)의 영향을 덜 받습니다. |
    | **최빈값(Mode)** | 데이터에서 가장 빈도수가 높은 값으로, 자료의 대표적인 특성을 보여줍니다. |
2. **산포도 (Dispersion)**
    
    
    | 구분 | 설명 |
    | --- | --- |
    | **범위(Range)** | 최댓값과 최솟값의 차이를 통해 자료가 퍼져 있는 정도를 간단히 확인합니다. |
    | **분산(Variance)** | 데이터 값들이 평균에서 얼마나 떨어져 있는지를 수치화한 지표로, 값이 클수록 변동성이 큽니다. |
    | **표준 편차(Standard Deviation)** | 분산의 제곱근으로, 데이터의 평균적 변동성을 직관적으로 파악하는 데 쓰입니다. |
    | **사분위 범위(IQR)** | 1사분위수(Q1)와 3사분위수(Q3)의 차이로, 데이터 중간 영역의 퍼짐 정도를 나타냅니다. |
3. **분포 특성 (Distribution Characteristics)**
    
    
    | **구분** | **설명** |
    | --- | --- |
    | **왜도(Skewness)** | 분포의 비대칭성을 나타내는 값입니다.
    0보다 크면 오른쪽으로 치우친 분포(양의 왜도), 0보다 작으면 왼쪽으로 치우친 분포(음의 왜도)를 뜻합니다. |
    | **첨도(Kurtosis)** | 분포가 뾰족한 정도로, 3보다 크면 정규 분포보다 뾰족한 분포(급첨), 3보다 작으면 평평한 분포(완만)를 의미합니다. |
- 사용 방법: SciPy의 `stats` 모듈 및 Pandas의 `.describe()`을 활용하여 데이터의 중심 경향성, 산포도, 분포 특성을 정량적으로 분석하고 요약하고 시각화

(다시 보기)

### 가설 검정 (Hypothesis Testing)

가설 검정은 표본을 기반으로 통계적 가설의 참과 거짓을 검정하여 결론을 도출하는 과정

- 비타민을 먹으면 집중력이 향상될까?

귀무가설**(Null Hypothesis, H₀)**: “비타민을 먹어도 집중력에는 차이가 **없다.”**

대립가설**(Alternative Hypothesis, H₁)**: “비타민을 먹으면 집중력이 **향상된다(있다).”**

→ 표본 데이터를 기반으로 검정 통계량 (Test Statistic)을 계산하고, 계산 값을 확률적으로 해석하여 귀무가설(차이 없다)을 **기각할지 결정**한다. 

귀무가설은 기본적으로 참이라고 가정하는 가설로, 연구자가 반증하고자 하는 대상. 

대립가설은 검정하고자 하는 가설(희망)이다. “변화가 있다”, “차이가 있다”

- 1종 오류: 실제로는 ‘귀무가설’이 참인데, 이를 잘못 기각하는 오류
- 2종 오류: 실제로는 ‘대립가설’이 참인데, 귀무가설을 기각하지 않는 오류

| **임계값 (Critical Value)** | 임계값은 검정 통계량이 귀무가설을 기각할지 결정하는 기준 값입니다.
특정 유의수준에서 계산되며, 검정 통계량이 이 값을 초과하면 귀무가설을 기각합니다.
예를 들어, **유의수준 0.05에서 t-검정의 임계값이 ±2.045라면, t값이 이 범위를 넘어갈 경우 귀무가설을 기각합니다.** |
| --- | --- |
| **p값 (p-value)** | p값은 귀무가설이 참일 때 현재 표본보다 극단적인 결과가 나올 확률을 의미합니다.
p값이 유의수준보다 작으면 귀무가설을 기각하고, 크면 귀무가설을 유지합니다.
예를 들어, **p값이 0.03이면 "이 결과가 우연히 발생할 확률이 3%밖에 안 되므로, 차이가 있다고 보겠다"는 결론을 내릴 수 있습니다.** |

### 통계적 시각화

블라블라블라..

# 한 줄 정리

- 시각화 : 데이터를 직관적으로 이해하기 위해 점, 선, 도형으로 표현하는 것
- 정형 데이터: 행과 열의 구조를 가지는 데이터
- 비정형 데이터: 텍스트, 이미지, 오디오, 영상 등 정해진 구조 없이 생산되는 데이터
- 히스토그램: 연속형 데이터가 각 구간에 얼마나 속해있는지 막대로 나타내는 시각화
- 박스 플롯: 데이터의 중간 50%를 박스로 나타내고 중앙값, 사분위수, 이상치를 알 수 있는 시각화
- 범주형 데이터: 명목형 혹은 순서형 범주로 묶일 수 있는 데이터
- 연속형 데이터: 무한한 중간값을 가질 수 있는 수치형 데이터
- 시계열 데이터: 시간이 흐름에 따라 측정된 연속적인 데이터
- 이동 평균: 데이터의 장기적인 추세를 관찰하고 싶을 때, 최근치의 평균을 계산해 다시 활용하는 것

⇒ OPEN API 활용해 matplotlib, seaborn, scipy 이용해 시각화 해보기