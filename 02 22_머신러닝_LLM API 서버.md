# 02.22_ë¨¸ì‹ ëŸ¬ë‹_LLM  API ì„œë²„

https://brunch.co.kr/@publichr/128

# ì¤€ë¹„

OpenAI API Key ë°œê¸‰

Tavily API Key ë°œê¸‰

ë­ì²´ì¸

https://python.langchain.com/docs/introduction/#tutorials

ì„ íƒ: ë­ìŠ¤ë¯¸ìŠ¤ ê°€ì…

# ê¸°ë³¸

https://brunch.co.kr/@publichr/128

ì£¼ëª©: ì½”ë“œë¡œ í”„ë¡¬í”„íŒ…ì„ í•œë‹¤. 

```python
pip install langchain
```

í™˜ê²½ ë³€ìˆ˜: ìš´ì˜ì²´ì œ ìˆ˜ì¤€ì—ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì´ë‹¤. 

ì˜ˆ) ìœ ë‹‰ìŠ¤ ì‰˜(í„°ë¯¸ë„)

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

*ê¸°ë³¸ì ìœ¼ë¡œ â€˜ìœ ë‹‰ìŠ¤ ì‚¬ìš©ë²•â€™ì€ ì•Œì•„ì•¼ í•œë‹¤. AI ì—”ì§€ë‹ˆì–´ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´*

í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ìœ„ì˜ ì§ì ‘ ì„¤ì •ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤(ëŒ€ì‹  ë‹·ì¸ë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“´ë‹¤)

í™˜ê²½ì„¤ì •ì„ .env íŒŒì¼ì— ì €ì¥í•´ë‘ê³  ê·¸ ì„¤ì •ì„ ì½ì–´ë“¤ì¸ë‹¤. 

```python
OPENAI_API_KEY="YOUR_KEY"
TAVILY_API_KEY="Y"
```

ë‹·ì¸ë¸Œë¥¼ ì„¤ì¹˜í•œë‹¤. 

pip install -qU "langchain[openai]â€

í™˜ê²½ì„¤ì •ì„ .env ì— ì €ì¥í•´ë‘ê³  ê·¸ ì„¤ì •ì„ ì½ì–´ë“¤ì¸ë‹¤ (ì´ë•Œ ì“°ëŠ”ê²Œ â€˜**dotenv**â€™)

pip install dotenv 

```python
from dotenv import load_dotenv

load_dotenv

```

ë§¨ ì•ì´ . ìœ¼ë¡œ ì‹œì‘í•˜ë©´ â†’ hidden file

.envëŠ” ë²„ì „ ê´€ë¦¬ ëŒ€ìƒì—ì„œ ì œì™¸í•´ì•¼í•œë‹¤.(ì¤‘ìš” ì •ë³´ ë…¸ì¶œ ìœ„í—˜)

.gitignoreì— .envë¥¼ ì¶”ê°€í•œë‹¤.

ì½”ë©ì—ì„œ í•  ë•ŒëŠ” 

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

ì´ë ‡ê²Œ í•´ë„ ë¨. 

ëª¨ë¸ ì„ íƒ: OpenAI

ëª¨ë¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤.

pip install -qU "langchain[openai]â€

í”„ë¡¬í”„íŠ¸ì— ë©”ì‹œì§€ ì—­í• ì´ ì—¬ëŸ¬ê°€ì§€ ìˆë‹¤. 

<aside>
ğŸ’¡

<Role>

system: ë§¥ë½ê³¼ ëª…ë ¹. ì–´ë–»ê²Œ í–‰ë™í•˜ê³ , ì¶”ê°€ì ì¸ context(ë§¥ë½)ì„ ì œê³µí•´ì¤Œ. 

- ëª¨ë“  ì±„íŒ… ëª¨ë¸ì´ ì œê³µí•˜ëŠ” ê±´ ì•„ë‹˜. openAIëŠ” ì œê³µí•¨.

user: ì‹¤ì œ ë‚´ìš©. ë²ˆì—­í•  ëŒ€ìƒì„ userê°€ ì…ë ¥í•˜ëŠ” ê²ƒ. 

assistant: userë¡œë¶€í„° ì–´ë–¤ ì‹ìœ¼ë¡œ ë„ì›€ì´ ì™”ìœ¼ë©´ ì¢‹ê² ë‹¤. (ex: ë„ˆëŠ” OO ë„ìš°ë¯¸ì•¼.)

tool: third party ë¶€ë¥¼ ë•Œ - (ì˜ˆì „ì—” functionì´ ì´ ê¸°ëŠ¥ì„ í–ˆìŒ)

LANGCHAINì€ ì—¬ëŸ¬ LLM ì„ ë‹¤ë£¨ê¸° ë•Œë¬¸ì—, ì´ê²Œ í‘œì¤€ì´ê³  ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. 

</aside>

## ì„œë²„(API)í™”

GET ìš”ì²­ì— ë¶€ê°€ ì •ë³´ë¡œ ê²½ë¡œ íŒŒë¼ë¯¸í„° ì™¸ì— ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤.

ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: ê²½ë¡œ ë’¤ì— ?key1=value1&key2=value2

ì˜ˆ) https://â€¦../say?text=hi

ì„œë²„ ì‹¤í–‰ì„ í•œë‹¤.

fastapi dev [server.py](http://server.py/)

í’€ìŠ¤íƒê³¼ ì†Œí†µí•  ë•Œ ë‹¤ìŒê³¼ ê°™ì´ ë§í•œë‹¤. 

GETìœ¼ë¡œ ê²½ë¡œëŠ” translate ì´ê³ , ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜ëŠ” textì™€ languageë¡œ ì£¼ì„¸ìš”. 

ì˜ˆì œ )[https://solid-xylophone-wrprrgw4j4x3g47q-8000.app.github.dev/translate?text=hi&language=Chinese](https://solid-xylophone-wrprrgw4j4x3g47q-8000.app.github.dev/translate?text=hi&language=Chinese)

ìŠ¤íŠ¸ë¦¬ë°

SSE: Server-Side Event ì›¹ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ì†ŒìŠ¤ë¥¼ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì—°ê²°í•˜ê³  ì„œë²„ëŠ” ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ë‚´ë ¤ì¤€ë‹¤. 

í˜„ì¬ êµ¬ì¡°: langchain LLM + fastapi ì„œë²„

---

```python
from fastapi import FastAPI, Query
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles

import app_model

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

model = app_model.AppModel()

@app.get("/say")
def say_app(text: str = Query()):
    response = model.get_response(text)
    return {"content" :response.content}

@app.get("/translate")
def translate(text: str = Query(), language: str = Query()):
    response = model.get_prompt_response(language, text)
    return {"content" :response.content}

@app.get("/says")
def say_app_stream(text: str = Query()):
    def event_stream():
        for message in model.get_streaming_response(text):
            yield f"data: {message.content}\n\n"
            
    return StreamingResponse(event_stream(), media_type="text/event-stream")
```

(ì—¬ê¸°ê¹Œì§€ ì˜¤ì „ ë‚´ìš©)

app.py

```python

from dotenv import load_dotenv
load_dotenv()

from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4o-mini", model_provider="openai")

from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage("Translate the follwing from English into Italian"),
    HumanMessage("hi!")
]

# response = model.invoke(messages)
# print(model.invoke(messages))

# for token in model.stream(messages):
#     print(token.content, end="|")

from langchain_core.prompts import ChatPromptTemplate

system_template = "Translate the following from English into {language}"

prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)

prompt = prompt_template.invoke({"language": "Korean", "text": "hi!"})
response = model.invoke(prompt)
print(response.content)
```

app_model.py

```python
from dotenv import load_dotenv

from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

class AppModel:
  def __init__(self):
    load_dotenv() 
    self.model = init_chat_model("gpt-4o-mini", model_provider="openai")
    system_template = "Translate the following from English into {language}"
    self.prompt_template = ChatPromptTemplate.from_messages(
      [("system", system_template), ("user", "{text}")]
    )

  def get_response(self, message):
    return self.model.invoke([HumanMessage(message)])

  def get_prompt_response(self, language, message):
    prompt = self.prompt_template.invoke({"language": language, "text": message})
    return self.model.invoke(prompt)

  def get_streaming_response(self, messages):
    return self.model.stream(messages) #asteamì€ ë¹„ë™ê¸°, streamì€ ë™ê¸°
```

---

# ì±—ë´‡

ë­ê·¸ë˜í”„ ì‚¬ìš©

https://python.langchain.com/docs/tutorials/chatbot/

ë©”ëª¨ë¦¬ (ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ë©´ì„œ ì†Œí†µ)

<aside>
ğŸ’¡

ê¸¸ì´ ì œí•œ ë¬¸ì œ, token ì œí•œ(?) ì´í•´í•˜ê¸°

</aside>

```python
# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
from dotenv import load_dotenv
load_dotenv()

# ëª¨ë¸ ì´ˆê¸°í™” 
from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4o-mini", model_provider="openai")

from langchain_core.messages import HumanMessage

# response = model.invoke([HumanMessage(content="Hi! I'm dain")])
# print(response.content) # í•­ìƒ ë°˜í™˜ê°’ì„ ë°›ì•„ì„œ print í•´ì£¼ê¸°

# response = model.invoke([HumanMessage(content="What's my name?")])
# print(response.content)

from langchain_core.messages import AIMessage

response = model.invoke(
    [
        HumanMessage(content="Hi! I'm dain"),
        AIMessage(content="Hello dain! How can I assist you today?"),
        HumanMessage(content="What's my name?"),
        AIMessage(content="Your name is Dain. How can I help you today?"),
        HumanMessage(content="openai API í˜¸ì¶œí•´ì„œ LLM ëª¨ë¸ì„ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ëŠ” ê³¼ì •ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜. ë‚˜ëŠ” ì½”ë“œìŠ¤í˜ì´ìŠ¤ì—ì„œ ì‘ì—…í•˜ê³  ìˆê³ , ì„œë²„ë‚˜ http ê°™ì€ ë°°ê²½ì§€ì‹ì´ ì—†ì–´. ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì¤˜." )
    ]
)
print(response.content)
```

- ëŒ€í™” ë‚´ìš© ë°±ì—…í•´ë†“ê¸° - ì„œë²„ êº¼ë„ ë‚ ì•„ê°€ì§€ ì•Šë„ë¡

LangGraphëŠ” ë‚´ì¥ëœ ì§€ì†ì„± ê³„ì¸µì„ êµ¬í˜„í•˜ì—¬ ì—¬ëŸ¬ ë²ˆì˜ ëŒ€í™” ì „í™˜ì„ ì§€ì›í•˜ëŠ” ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì´ìƒì ì…ë‹ˆë‹¤. ìµœì†Œí•œì˜ LangGraph ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ ì±„íŒ… ëª¨ë¸ì„ ê°ì‹¸ë©´ ë©”ì‹œì§€ ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ë‹¤ì¤‘ ì „í™˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangGraphì—ëŠ” ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°ê°€ ì œê³µë˜ë©°, ì•„ë˜ì—ì„œ ì´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì†ì„± ë°±ì—”ë“œ(ì˜ˆ: SQLite ë˜ëŠ” Postgres)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ë“± ìì„¸í•œ ë‚´ìš©ì€ í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define the function that calls the model
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}

# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

- ìƒˆë¡œìš´ ëŒ€í™” í•˜ê³  ì‹¶ìœ¼ë©´, thread_idë¥¼ ë‹¤ë¥´ê²Œ í•˜ë©´ ë¨

```python
config = {"configurable": {"thread_id": "abc123"}}
```

app = LangGraph application

app.invoke() í•˜ë©´ ê·¸ë ¤ë†“ì€ Graph ë”°ë¼ì„œ ì²˜ë¦¬ê°€ ë˜ëŠ” ê²ƒ. 

```python
# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
from dotenv import load_dotenv
load_dotenv()

# ëª¨ë¸ ì´ˆê¸°í™” 
from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4o-mini", model_provider="openai")

from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define the function that calls the model
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}

# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {"configurable": {"thread_id": "abc123"}}

query = "Hi! I'm dain."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # output contains all messages in state

query = "What's my name?"

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

â†’ ì•„ê¹Œì™€ ë‹¬ë¦¬, ë‹¨ë°œì„± ë©”ì‹œì§€ë¥¼ ë‘ ë²ˆ ë³´ë‚´ë„ LangGraphì— ì €ì¥ë¼ì„œ ì—°ê²°ëœë‹¤. 

- ë©€í‹° ìœ ì €

```python
config = {"configurable": {"thread_id": "abc123"}}

query = "What's my name?"

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()

config = {"configurable": {"thread_id": "abc234"}}

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

- ì»¤ìŠ¤í…€(State) ìƒíƒœ ì •ì˜í•œ ì½”ë“œ + trimmer â‡’ ìµœì¢… ì½”ë“œ

```python
# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
from dotenv import load_dotenv
load_dotenv() 

# ëª¨ë¸ ì´ˆê¸°í™”
from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4o-mini", model_provider="openai")

from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# í”„ë¡¬í”„íŠ¸ ìƒì„±: ì–¸ì–´ ë²ˆì—­
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability in {language}.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

from langchain_core.messages import AIMessage, SystemMessage, trim_messages

trimmer = trim_messages(
    max_tokens=65,
    strategy="last",
    token_counter=model,
    include_system=True,
    allow_partial=False,
    start_on="human",
)

messages = [
    SystemMessage(content="you're a good assistant"),
    HumanMessage(content="hi! I'm bob"),
    AIMessage(content="hi!"),
    HumanMessage(content="I like vanilla ice cream"),
    AIMessage(content="nice"),
    HumanMessage(content="whats 2 + 2"),
    AIMessage(content="4"),
    HumanMessage(content="thanks"),
    AIMessage(content="no problem!"),
    HumanMessage(content="having fun?"),
    AIMessage(content="yes!"),
]

trimmer.invoke(messages)

from typing import Sequence

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict

# ì»¤ìŠ¤í…€ ìƒíƒœ ì •ì˜: ì–¸ì–´ ì…ë ¥
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str

# Define a new graph(ìŠ¤í‚¤ë§ˆëŠ” ìš°ë¦¬ê°€ ì •ì˜í•œ State)
workflow = StateGraph(state_schema=State)

# Define the function that calls the model. ì¸ìˆ˜ëŠ” ìš°ë¦¬ê°€ ì •ì˜í•œ State
def call_model(state: State):
    trimmed_messages = trimmer.invoke(state["messages"])
    prompt = prompt_template.invoke(
        {"messages": trimmed_messages, "language": state["language"]}
    )
    response = model.invoke(prompt)
    return {"messages": response}

# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {"configurable": {"thread_id": "abc678"}}
query = "What math problem did I ask?"
language = "English"

input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

<ì¶”ê°€ë¯¸ì…˜>

í´ë˜ìŠ¤ë¡œ ë§Œë“ ë‹¤. 

API ì„œë²„ë¡œ ë§Œë“ ë‹¤. (server.pyì— ë„£ìœ¼ë©´ ë¨) â†’ githubì— ì˜¬ë ¤ì¤„ ì˜ˆì •. ì°¸ê³ í•˜ê¸”

[chatbot.py](http://chatbot.py) â†’ chatbot_model.py (app_model.py ì°¸ê³ ) â†’ server.pyì—ì„œ chatbot_model.pyë¥¼ ì¨ì„œ GET ì—”ë“œí¬ì¸íŠ¸ ì œê³µ

# ì—ì´ì „íŠ¸

= ëŒ€í™”ë§Œ í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ë” ë§ì€ ê¸°ëŠ¥. 

- ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ (Tavily í™œìš©)

ex) ë‚ ì”¨ë¥¼ ë¬¼ì–´ë³´ë©´, LLM ì´ ë‚ ì”¨ë¥¼ ì°¾ì•„ì£¼ê¸° ì–´ë ¤ì›€. â€˜ê²€ìƒ‰â€™ì„ í™œìš©í•´ì„œ ë” ë§ì€ ê¸°ëŠ¥ì„ ì œê³µí•  ìˆ˜ ìˆëŠ” ê²ƒ. 

<ì¶”ê°€ë¯¸ì…˜>

í´ë˜ìŠ¤ë¡œ ë§Œë“ ë‹¤.

API ì„œë²„ë¡œ ë§Œë“ ë‹¤.(ì¼ë°˜ GET ìš”ì²­ ì‘ë‹µ)

RAG - ë§ˆì§€ë§‰ ì£¼ì°¨ ì¦ˆìŒ..

# ì¶”í›„

- ë©€í‹°ëª¨ë‹¬

https://livekit.io/